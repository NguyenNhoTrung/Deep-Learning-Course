{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"trainingNN_practice_pytorch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","source":["\n","\n","# **Assignment: Training neural network: Learning rate, dropout, activation function**\n","\n","Trong bài thực hành này, chúng ta sẽ tìm hiểu các vấn đề về learning rate, dropout, activation function thông qua bài toán phân loại chữ số viết tay trên bộ dữ liệu MNIST."],"metadata":{"id":"L3yJG9Fr4rlZ"}},{"cell_type":"code","execution_count":null,"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","import torch\n","import os\n","from torch import nn\n","import torch.nn.functional as F \n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","%load_ext tensorboard"],"outputs":[],"metadata":{"id":"PH2NM_aU4rlb"}},{"cell_type":"markdown","source":["## **Phần 1: Quan sát dữ liệu**\n","MNIST là tập dữ liệu ảnh đen trắng các chữ số viết tay, có cùng kích thước 28x28. Chữ số trong ảnh đã được căn chỉnh vào tâm. Đây là tập dữ liệu rất phù hợp cho việc thử nghiệm các kỹ thuật huấn luyện và nhận dạng mẫu mà không đòi hỏi quá nhiều công sức tiền xử lý.\n","\n","Bộ dữ liệu MNIST được chia sẵn thành 2 phần: tập dữ liệu huấn luyện gồm 60.000 ảnh, tập dữ liệu kiểm thử gồm 10.000 ảnh. Các ảnh trong bộ dữ liệu thuộc về một trong 10 lớp: 0, 1, 2,..., 9.\n","\n","Thư viện PyTorch đã cung cấp sẵn một module để tải về MNIST:"],"metadata":{"id":"V13ZEq0L4rlf"}},{"cell_type":"code","execution_count":null,"source":["train_data = datasets.MNIST(\n","    root = 'data',\n","    train = True,                         \n","    download = True        \n",")\n","test_data = datasets.MNIST(\n","    root = 'data', \n","    train = False\n",")\n","(x_train, y_train) = train_data.data[:].detach().numpy(), train_data.targets[:].detach().numpy()\n","(x_test, y_test) = test_data.data[:].detach().numpy(), test_data.targets[:].detach().numpy()\n","print('Training image: ', x_train.shape)\n","print('Testing image: ', x_test.shape)\n","print('Training label: ', y_train.shape)\n","print('Testing label: ', y_test.shape)\n"],"outputs":[],"metadata":{"id":"X9tPOvTxweU0"}},{"cell_type":"markdown","source":["Để dễ hình dung về dữ liệu, có thể sử dụng thư viện matplotlib quan sát một vài mẫu dữ liệu:"],"metadata":{"id":"ZWqAN9fx4rlj"}},{"cell_type":"code","execution_count":null,"source":["for i in range(30):\n","    idx = np.random.randint(0, x_train.shape[0])\n","    image = x_train[idx]\n","    plt.subplot(3, 10, i + 1), plt.imshow(image, cmap='gray')\n","    plt.title(y_train[idx]), plt.xticks([]), plt.yticks([])\n","plt.show()"],"outputs":[],"metadata":{"id":"zLCmxJhB4rlk"}},{"cell_type":"code","execution_count":null,"source":["batch_size = 64\n","num_classes = 10\n","epochs = 20\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","input_shape = (img_rows, img_cols, 1)"],"outputs":[],"metadata":{"id":"VlkbkIB4dhnL"}},{"cell_type":"markdown","source":["Trước khi đưa vào mô hình, cần chuẩn hoá giá trị điểm ảnh về trong khoảng [0,1] nhằm giúp các thuật toán tối ưu hội tụ nhanh hơn:"],"metadata":{"id":"VFdZU2UA4rlm"}},{"cell_type":"code","execution_count":null,"source":["x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255"],"outputs":[],"metadata":{"id":"tgfwdoMQ4rln"}},{"cell_type":"code","execution_count":null,"source":["print(x_train.shape, y_train.shape)"],"outputs":[],"metadata":{"id":"-yDI-nkqzGwF"}},{"cell_type":"markdown","source":["## Phần 2: Xây dựng mô hình phân loại\n","\n","Trong phần này, chúng ta sẽ xây dựng một mô hình phân loại đơn giản cho bài toán nhận diện chữ số viết tay sử dụng thư viện keras. Từ mô hình này ta sẽ thử nghiệm tác động của các yếu tố như learning rate, dropout, activation function đến quá trình huấn luyện mạng. "],"metadata":{"id":"j41BaPWk4rlp"}},{"cell_type":"code","execution_count":null,"source":["class DeepModel(nn.Module):\n","    def __init__(self, dropout_rate):\n","        super(DeepModel, self).__init__()\n","        self.conv_1 = nn.Conv2d(1, 32, kernel_size=3, stride=(1, 1), padding=0, bias=False, dilation=1)\n","        self.conv_2 = nn.Conv2d(32, 64, kernel_size=3, stride=(1, 1), padding=0, bias=False, dilation=1)\n","        self.maxpool = nn.MaxPool2d(kernel_size=2)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        self.dense_1 = nn.Linear(12*12*64, 512)\n","        self.dense_2 = nn.Linear(512, 10)\n","    \n","    def forward(self, x):\n","        \n","        x = x.permute(None, None, None, None) # reshape input to (batch, #channels, height, width)\n","        ### START CODE HERE ~ 5-9 lines\n","        ## Network Architecture: 2x(Conv2d -> ReLU) -> MaxPool2d -> Flatten -> Linear -> ReLU -> Dropout -> Linear -> Log Softmax (implemented)\n","        \n","        ### END CODE HERE\n","        output = F.log_softmax(x)\n","        return output"],"outputs":[],"metadata":{"id":"2bifJlWBzYxp"}},{"cell_type":"markdown","source":["#### Kiểm tra số lượng tham số"],"metadata":{"id":"1XC21qEu-Z8x"}},{"cell_type":"code","execution_count":null,"source":["simple_model = DeepModel(dropout_rate = 0.5).cuda()\n","for param in simple_model.parameters():\n","    if param.requires_grad:\n","        print('param autograd')\n","        break\n","\n","input = torch.rand(2, 28, 28, 1).cuda()\n","output = simple_model(input)  # type: torch.Tensor\n","\n","model_parameters = filter(lambda p: p.requires_grad, simple_model.parameters())\n","params = sum([np.prod(p.size()) for p in model_parameters])\n","print('Number of parameter:', params)\n","\n","assert params==4742954, \"Kiểm tra lại phần forward\""],"outputs":[],"metadata":{"id":"melCv4vo3URb"}},{"cell_type":"markdown","source":["####  Khởi tạo Generator"],"metadata":{"id":"YbIb7b109Oim"}},{"cell_type":"code","execution_count":null,"source":["class Generator(Dataset):\n","    def __init__(self, images, labels):\n","        self.images = images\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return self.images.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.images[idx], self.labels[idx]"],"outputs":[],"metadata":{"id":"FRgI7eV29NaQ"}},{"cell_type":"code","execution_count":null,"source":["training_data = Generator(x_train, y_train)\n","train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)"],"outputs":[],"metadata":{"id":"i5Rw4gdA9UM3"}},{"cell_type":"code","execution_count":null,"source":["test_data = Generator(x_test, y_test)\n","test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)"],"outputs":[],"metadata":{"id":"h4JWTVMu9V_4"}},{"cell_type":"code","source":["# Iterate through the Dataloader\n","data, label = next(iter(train_dataloader))\n","print(f\"Feature batch shape: {data.size()}\")\n","print(f\"Labels batch shape: {label.size()}\")\n","\n","# Plot\n","img = data[0].squeeze()\n","label = label[0]\n","plt.imshow(img, cmap=\"gray\")\n","plt.show()\n","print(f\"Label: {label}\")"],"metadata":{"id":"M11yuisRbjo9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Phần 2.1: Vai trò của Dropout \n","\n","Kỹ thuật dropout tắt đi một số kết nối một cách ngẫu nhiên trong mỗi lượt huấn luyện, giúp tránh hiện tượng đồng thích nghi (co-adaptation). Điều đó giúp mô hình bị quá khớp. Việc quá khớp thể hiện ở việc lỗi huấn luyện là rất nhỏ nhưng lỗi kiểm thử lại lớn. Phần thực hành tiếp theo nhằm minh hoạ cho tác dụng của kỹ thuật này.\n","\n","Nếu 1 lớp fully connected có quá nhiều tham số và chiếm hầu hết tham số, các nút mạng trong lớp đó quá phụ thuộc lẫn nhau trong quá trình huấn luyện thì sẽ hạn chế sức mạnh của mỗi nút, dẫn đến việc kết hợp quá mức.\n","\n","![dropout](https://images.viblo.asia/b621cd15-1862-465e-b11e-6655f3f855c0.png)\n","\n","Tham khảo: https://towardsdatascience.com/introduction-to-dropout-to-regularize-deep-neural-network-8e9d6b1d4386"],"metadata":{"id":"Doqa-Juc4rls"}},{"cell_type":"markdown","source":["#### Dropout rate = 0"],"metadata":{"id":"qOcc-Xhx4rlt"}},{"cell_type":"code","execution_count":null,"source":["use_cuda = torch.cuda.is_available()  #GPU cuda\n","best_loss = float('inf')\n","\n","model = DeepModel(dropout_rate = 0)\n","\n","optimizer = torch.optim.Adam(model.parameters())\n","if use_cuda:\n","    model = torch.nn.parallel.DataParallel(model.cuda())   # , device_ids=[0, 1, 2, 3]\n","    torch.backends.cudnn.benchmark = True"],"outputs":[],"metadata":{"id":"JXUSYfTJ9gI0"}},{"cell_type":"code","execution_count":null,"source":["def train(model, epoch, writer):\n","    print('\\n ############################# Train phase, Epoch: {} #############################'.format(epoch))\n","    model.train()\n","    train_loss = 0\n","    running_loss = 0\n","    print('\\nLearning rate at this epoch is: ', optimizer.param_groups[0]['lr'], '\\n')\n","    for (batch_idx, target_tuple) in enumerate(train_dataloader):\n","        if use_cuda:\n","            target_tuple = [target_tensor.cuda(non_blocking=True) for target_tensor in target_tuple]\n","\n","        images, labels = target_tuple\n","        # Convert label to long type pytorch\n","        labels = torch.tensor(labels,dtype=torch.long)\n","\n","        optimizer.zero_grad()  # zero the gradient buff\n","        output_tuple = model(images)\n","\n","        loss = F.nll_loss(output_tuple, labels).cuda()\n","\n","        loss.backward()  # retain_graph=True\n","        optimizer.step()\n","\n","        train_loss += loss.item()  # loss　　　\n","        running_loss += loss.item()\n","        if batch_idx % 50 == 49:\n","            writer.add_scalar('training loss', running_loss/50, epoch * len(train_dataloader) + batch_idx)\n","            running_loss = 0\n","        #print('########################### Epoch:', epoch, ', --  batch:',  batch_idx, '/', len(train_dataloader), ',   ',\n","        #      'Train loss: %.3f, accumulated average loss: %.3f ##############################' % (loss.item(), train_loss / (batch_idx + 1)))\n","\n"],"outputs":[],"metadata":{"id":"ZOElRkpR9zKC"}},{"cell_type":"code","execution_count":null,"source":["def test(model, epoch, writer):\n","    print('\\n ############################# Test phase, Epoch: {} #############################'.format(epoch))\n","    model.eval()\n","    with torch.no_grad():\n","        test_loss = 0\n","        correct = 0\n","        for (batch_idx, target_tuple) in enumerate(test_dataloader):\n","            if use_cuda:\n","                target_tuple = [target_tensor.cuda(non_blocking=True) for target_tensor in target_tuple]\n","\n","            images, labels = target_tuple\n","            # Convert label to long type pytorch\n","            labels = torch.tensor(labels,dtype=torch.long)\n","            output_tuple = model(images)\n","            #print(output_tuple.shape)\n","\n","            _, predicted = torch.max(output_tuple.data, 1)\n","            correct += (predicted == labels).sum().item()\n","\n","            loss = F.nll_loss(output_tuple, labels).cuda()\n","\n","            test_loss += loss.item()  # loss　　　\n","            #print('########################### Epoch:', epoch, ', --  batch:',  batch_idx, '/', len(test_dataloader), ',   ',\n","            #     'Test loss: %.3f, accumulated average loss: %.3f ##############################' % (loss.item(), test_loss / (batch_idx + 1)))\n","        acc = correct*100/len(test_data)\n","        print('Accuracy: ', acc)\n","        writer.add_scalar('test accuracy', acc, epoch)\n","\n"],"outputs":[],"metadata":{"id":"fEFvQKvt92kE"}},{"cell_type":"code","execution_count":null,"source":["def train_and_test(model, epoch_num = 5, summary_path='runs/mnist_experiment_dropout'):\n","    writer = SummaryWriter(summary_path)\n","    for epoch in range(epoch_num):\n","        train(model, epoch, writer)\n","        test(model, epoch, writer)"],"outputs":[],"metadata":{"id":"ejyHFR4zfcVP"}},{"cell_type":"markdown","source":["Huấn luyện mô hình"],"metadata":{"id":"a4iWxR8PVN2T"}},{"cell_type":"code","execution_count":null,"source":["train_and_test(model, 5, 'runs/mnist_experiment_dropout=0')"],"outputs":[],"metadata":{"id":"5VknBhoq946p"}},{"cell_type":"code","execution_count":null,"source":["%tensorboard --logdir=runs"],"outputs":[],"metadata":{"id":"Q2yrTbsFLeHD"}},{"cell_type":"markdown","source":["#### Dropout rate = 0.5"],"metadata":{"id":"-hEyE73u4rlw"}},{"cell_type":"code","execution_count":null,"source":["use_cuda = torch.cuda.is_available()  #GPU cuda\n","best_loss = float('inf')\n","\n","### START CODE HERE ~ 1 lines\n","## Create an instance of DeepModel with dropout = 0.5 and load it to cuda()\n","\n","### END CODE HERE\n","\n","optimizer = torch.optim.Adam(model.parameters())\n","# if use_cuda:\n","#     model = torch.nn.parallel.DataParallel(model.cuda())   # , device_ids=[0, 1, 2, 3]\n","#     torch.backends.cudnn.benchmark = True"],"outputs":[],"metadata":{"id":"hki-juDC4rlx"}},{"cell_type":"code","execution_count":null,"source":["train_and_test(model, optimizer, 5, 'runs/mnist_experiment_dropout=0.5')"],"outputs":[],"metadata":{"id":"qRUoy1y0Fywy"}},{"cell_type":"code","execution_count":null,"source":["%tensorboard --logdir=runs"],"outputs":[],"metadata":{"id":"2s_U4n3fUULT"}},{"cell_type":"markdown","source":["#### **Nhận xét và chú ý**\n","Nhận xét:\n","- Dropout sẽ được học thêm các tính năng mạnh mẽ hữu ích\n","- Nó gần như tăng gấp đôi số epochs cần thiết để hội tụ. Tuy nhiên, thời gian cho mỗi epoch là ít hơn. Trong phần visualization, các bạn có thể thấy mặc dù mặc dù độ chính xác của phần dropout=0.5 thấp hơn lúc không áp dụng dropout nhưng trên trên tập test độ chính xác lại cao hơn, và nó cũng cần nhiều epoch hơn để hội tụ.\n","- Ta có H đơn vị ẩn, với xác suất bỏ học cho mỗi đơn vị là (1 - p) thì ta có thể có 2^H mô hình có thể có. Nhưng trong giai đoạn test, tất cả các nút mạng phải được xét đến, và mỗi activation sẽ giảm đi 1 hệ số p.\n","\n","Chú ý:\n","- Không dùng Dropout cho quá trình test\n","- Áp dụng Dropout cho cả quá trình Forward và Backward\n","- Giá trị kích hoạt phải giảm đi 1 hệ số keep_prob, tính cả cho những nút bỏ học."],"metadata":{"id":"ja_9zwOd-xbq"}},{"cell_type":"markdown","source":["### Phần 2.2: Vai trò của activation function\n","Để nói về vai trò của hàm kích hoạt, vậy hãy thử đặt ra câu hỏi: **Chuyện gì sẽ xảy ra nếu không có các hàm kích hoạt (hàm phi tuyến) này?**\n","\n","Hãy tưởng tượng rằng thay vì áp dụng 1 hàm phi tuyến, ta chỉ áp dụng 1 hàm tuyến tính vào đầu ra của mỗi neuron. Vì phép biến đổi không có tính chất phi tuyến, việc này không khác gì chúng ta thêm một tầng ẩn nữa vì phép biến đổi cũng chỉ đơn thuần là nhân đầu ra với các weights. Với chỉ những phép tính đơn thuần như vậy, trên thực tế mạng neural sẽ không thể phát hiện ra những quan hệ phức tạp của dữ liệu (ví dụ như: dự đoán chứng khoán, các bài toán xử lý ảnh hay các bài toán phát hiện ngữ nghĩa của các câu trong văn bản). Nói cách khác nếu không có các activation functions, khả năng dự đoán của mạng neural sẽ bị giới hạn và giảm đi rất nhiều, sự kết hợp của các activation functions giữa các tầng ẩn là để giúp mô hình học được các quan hệ phi tuyến phức tạp tiềm ẩn trong dữ liệu.\n","\n","Tham khảo: \n","- https://viblo.asia/p/mot-so-ham-kich-hoat-trong-cac-mo-hinh-deep-learning-tai-sao-chung-lai-quan-trong-den-vay-part-1-ham-sigmoid-bWrZn4Rv5xw\n","\n","- https://towardsdatascience.com/the-importance-and-reasoning-behind-activation-functions-4dc00e74db41#:~:text=Why%20do%20we%20need%20them,a*x%2Bb)."],"metadata":{"id":"ezkf-dZR4rlz"}},{"cell_type":"markdown","source":["Viết mô hình đơn giản cho phép nhận các loại hàm kích hoạt khác nhau"],"metadata":{"id":"QCR9ShAwVN2U"}},{"cell_type":"code","execution_count":null,"source":["class SimpleModel(nn.Module):\n","    def __init__(self, dropout_rate =0.5, activation = None):\n","        super(SimpleModel, self).__init__()\n","        self.dense_1 = nn.Linear(28*28, 512)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.dense_2 = nn.Linear(512, 10)\n","        self.activation = activation\n","    \n","    def forward(self, x):\n","        ### START CODE HERE ~ 6 lines\n","        ## Network Architecture: Flatten(input) -> Linear -> activation -> Dropout -> Linear -> Log Softmax\n","        ## The activation of the first Linear will be passed when creating an instance of model \n","\n","        ### END CODER HERE\n","        return output"],"outputs":[],"metadata":{"id":"nt57hVcm-Ks4"}},{"cell_type":"markdown","source":["Kiểm tra số lượng tham số\n"],"metadata":{"id":"etBUDYCA-O0T"}},{"cell_type":"code","execution_count":null,"source":["simple_model = SimpleModel().cuda()\n","for param in simple_model.parameters():\n","    if param.requires_grad:\n","        print('param autograd')\n","        break\n","\n","input = torch.rand(1, 28, 28).cuda()\n","output = simple_model(input)  # type: torch.Tensor\n","\n","model_parameters = filter(lambda p: p.requires_grad, simple_model.parameters())\n","params = sum([np.prod(p.size()) for p in model_parameters])\n","print('Number of parameter:', params)\n","\n","assert params==407050, \"Kiểm tra lại phần forward\""],"outputs":[],"metadata":{"id":"3smbjAkf-NEW"}},{"cell_type":"markdown","source":["Trước tiên hãy thử xem, sẽ thế nào nếu không sử dụng hàm kích hoạt cho lớp ẩn:"],"metadata":{"id":"M30NCbzHFEvX"}},{"cell_type":"code","execution_count":null,"source":["### START CODE HERE\n","simple_model = None # Do not forget pass model to cuda\n","optimizer = None # Adam optimizer\n","### ENCODE CODE HERE\n","train_and_test(simple_model, optimizer, 5, 'runs/mnist_none_activation')"],"outputs":[],"metadata":{"id":"XA9JRPv7FGnO"}},{"cell_type":"markdown","source":["Kết quả đạt được là tương đối tệ trên tập dữ liệu MNIST. Tiếp theo chúng ta sẽ thử nghiệm và so sánh kết quả khi sử dụng các hàm kích hoạt khác nhau:"],"metadata":{"id":"E0w0MxkEFob4"}},{"cell_type":"code","execution_count":null,"source":["for activation in [None, nn.Sigmoid(), nn.Tanh(), nn.ReLU()]:\n","    print(\"Activation: \", str(activation))\n","    ### START CODE HERE\n","    simple_model = None # Do not forget to pass model to cuda\n","    optimizer = None # Adam optimizer\n","    ### END CODE HERE\n","    train_and_test(simple_model, optimizer, 5, 'runs/mnist_' + str(activation))"],"outputs":[],"metadata":{"id":"IWdaI90K_gmf"}},{"cell_type":"code","execution_count":null,"source":["%tensorboard --logdir=runs"],"outputs":[],"metadata":{"id":"zK8xi9qoTuzR"}},{"cell_type":"markdown","source":["### Phần 2.3: Vai trò của hệ số học learning rate\n","\n","Ta tiếp tục quan sát tác động của learning rate đến quá trình học của mạng.\n","\n","Tham khảo: https://viblo.asia/p/learning-rate-nhung-dieu-co-the-ban-da-bo-qua-gGJ59BV9KX2"],"metadata":{"id":"8hSeHoop4rl-"}},{"cell_type":"code","execution_count":null,"source":["learning_rates = [1E-0, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5, 1E-6, 1E-7]\n","for lr in learning_rates:\n","    ### START CODE HERE\n","    print('Learning rate: %f' % lr)\n","    simple_model = None\n","    optimizer = None # SGD with momentum = 0.9\n","    None # train and test these models\n","    ### END CODE HERE"],"outputs":[],"metadata":{"id":"Tfij5YgHHMJG"}},{"cell_type":"code","execution_count":null,"source":["%tensorboard --logdir=runs"],"outputs":[],"metadata":{"id":"bE_DYon3W-Z9"}},{"cell_type":"markdown","source":["#### Giảm learning rate theo cơ chế: `learning_rate = init_learning_rate / (1 + decay * step)`"],"metadata":{"id":"HH22WJUQ4rmG"}},{"cell_type":"code","execution_count":null,"source":["def train(model, optimizer, scheduler, epoch, writer):\n","    print('\\n ############################# Train phase, Epoch: {} #############################'.format(epoch))\n","    model.train()\n","    train_loss = 0\n","    running_loss = 0\n","    print('\\nLearning rate at this epoch is: ', scheduler.get_last_lr(), '\\n')\n","    for (batch_idx, target_tuple) in enumerate(train_dataloader):\n","        if use_cuda:\n","            target_tuple = [target_tensor.cuda(non_blocking=True) for target_tensor in target_tuple]\n","\n","        images, labels = target_tuple\n","        # Convert label to long type pytorch\n","        labels = torch.tensor(labels,dtype=torch.long)\n","\n","        optimizer.zero_grad()  # zero the gradient buff\n","        output_tuple = model(images)\n","\n","        loss = F.nll_loss(output_tuple, labels).cuda()\n","\n","        loss.backward()  # retain_graph=True\n","        optimizer.step()\n","        \n","\n","        train_loss += loss.item()  # loss　　　\n","        running_loss += loss.item()\n","        if batch_idx % 50 == 49:\n","            writer.add_scalar('training loss', running_loss/50, epoch * len(train_dataloader) + batch_idx)\n","            running_loss = 0\n","        #print('########################### Epoch:', epoch, ', --  batch:',  batch_idx, '/', len(train_dataloader), ',   ',\n","        #      'Train loss: %.3f, accumulated average loss: %.3f ##############################' % (loss.item(), train_loss / (batch_idx + 1)))\n","    scheduler.step()"],"outputs":[],"metadata":{"id":"o4N0QKhtZLw2"}},{"cell_type":"code","execution_count":null,"source":["def train_and_test(model, optimizer, scheduler, epoch_num = 5, summary_path='runs/mnist_experiment_dropout'):\n","    writer = SummaryWriter(summary_path)\n","    for epoch in range(epoch_num):\n","        train(model, optimizer, scheduler, epoch, writer)\n","        test(model, epoch, writer)"],"outputs":[],"metadata":{"id":"jTHdi-Ryae76"}},{"cell_type":"markdown","source":["Viết đoạn code cho phép huấn luyện mô hình với tốc độ học giảm dần qua từng epoch với hệ số 0.9 sử dụng các schedule learning rate `ExponentialLR`"],"metadata":{"id":"EhAYGo-4VN2Y"}},{"cell_type":"code","execution_count":null,"source":["### START CODE HERE\n","simple_model = None # use ReLU, dropout = 0.5\n","optimizer = None # SGD\n","scheduler = None # ExponentialLR with gamma=0.9\n","### END CODE HERE\n","train_and_test(simple_model, optimizer, scheduler, 20, 'runs/\bexponentialLR')"],"outputs":[],"metadata":{"id":"ClZ6n53R4rmH"}},{"cell_type":"code","execution_count":null,"source":["%tensorboard --logdir=runs"],"outputs":[],"metadata":{"id":"q9ENpzJAZ6Ge"}}]}